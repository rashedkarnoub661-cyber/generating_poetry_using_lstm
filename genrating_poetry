# poetry_generation.py
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import random

# -----------------------------
# إعداد البيانات
# -----------------------------
class PoetryDataset(Dataset):
    def __init__(self, texts, seq_len=50):
        self.seq_len = seq_len
        tokens = " ".join(texts).split()
        self.vocab = {w: i for i, w in enumerate(set(tokens), start=2)}
        self.vocab["<PAD>"] = 0
        self.vocab["<UNK>"] = 1
        self.inv_vocab = {i: w for w, i in self.vocab.items()}
        self.data = [self.vocab.get(w, 1) for w in tokens]

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = self.data[idx:idx+self.seq_len]
        y = self.data[idx+1:idx+self.seq_len+1]
        return torch.tensor(x), torch.tensor(y)

# -----------------------------
# نموذج LSTM
# -----------------------------
class LSTMGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden=None):
        x = self.embed(x)
        out, hidden = self.lstm(x, hidden)
        out = self.fc(out)
        return out, hidden

# -----------------------------
# نموذج Transformer
# -----------------------------
class TransformerGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, num_heads=8, num_layers=4, ff_dim=512, max_len=500):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.pos_embed = nn.Embedding(max_len, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)
        x = self.embed(x) + self.pos_embed(positions)
        x = self.transformer(x)
        return self.fc(x)

# -----------------------------
# حلقة التدريب
# -----------------------------
def train_model(model, dataloader, vocab_size, epochs=10, lr=1e-3, device="cuda"):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    for epoch in range(1, epochs+1):
        total_loss = 0
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            if isinstance(model, LSTMGenerator):
                out, _ = model(x)
            else:
                out = model(x)
            loss = criterion(out.view(-1, vocab_size), y.view(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")

# -----------------------------
# توليد الشعر
# -----------------------------
def generate_text(model, dataset, start_text="love", length=50, device="cuda"):
    model.eval()
    words = start_text.split()
    idxs = [dataset.vocab.get(w, 1) for w in words]
    inp = torch.tensor(idxs, device=device).unsqueeze(0)

    hidden = None
    for _ in range(length):
        if isinstance(model, LSTMGenerator):
            out, hidden = model(inp, hidden)
        else:
            out = model(inp)
        next_token = torch.argmax(out[:, -1, :], dim=-1).item()
        words.append(dataset.inv_vocab.get(next_token, "<UNK>"))
        inp = torch.cat([inp, torch.tensor([[next_token]], device=device)], dim=1)
    return " ".join(words)

# -----------------------------
# مثال تشغيل
# -----------------------------
if __name__ == "__main__":
    # بيانات تجريبية (ضع هنا مجموعة أشعارك)
    texts = [
        "love is a flame that burns in the night",
        "the moon whispers softly to the sea",
        "dreams are woven in the fabric of time"
    ]

    dataset = PoetryDataset(texts, seq_len=10)
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

    vocab_size = len(dataset.vocab)

    # اختر النموذج: LSTM أو Transformer
    model = LSTMGenerator(vocab_size)   # أو TransformerGenerator(vocab_size)

    train_model(model, dataloader, vocab_size, epochs=20, device="cpu")

    poem = generate_text(model, dataset, start_text="love", length=30, device="cpu")
    print("\nGenerated Poem:\n", poem)
